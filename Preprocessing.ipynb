{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OcmirEzwjMX"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install bitsandbytes\n",
        "!pip install accelerate\n",
        "!pip install torch\n",
        "!pip install stanza\n",
        "!pip uninstall googletrans\n",
        "\n",
        "!pip install langdetect\n",
        "!pip install -U deep-translator\n",
        "!pip install googletrans==3.1.0a0\n",
        "!pip install google_trans_new\n",
        "\n",
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "import stanza\n",
        "from deep_translator import GoogleTranslator\n",
        "from googletrans import Translator\n",
        "import time\n",
        "import re\n",
        "import csv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fase di pre-processing\n",
        "Input:\n",
        "- File con le annotazioni\n",
        "- Cartella con gli articoli da analizzare"
      ],
      "metadata": {
        "id": "Qmhuip4xw-Ri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translated = GoogleTranslator(source='auto', target='de').translate(\"keep it up, you are awesome\")  # output -> Weiter so, du bist großartig\n",
        "print(translated)\n",
        "\n",
        "from googletrans import Translator\n",
        "def newTranslator(text):\n",
        "\n",
        "  translator = Translator()\n",
        "  ar = translator.translate(text).text\n",
        "  return ar\n"
      ],
      "metadata": {
        "id": "bFoEZSYoxHV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import re\n",
        "alphabets= \"([A-Za-z])\"\n",
        "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
        "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
        "starters = \"(Mr|Mrs|Ms|Dr|Prof|Capt|Cpt|Lt|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
        "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
        "websites = \"[.](com|net|org|io|gov|edu|me)\"\n",
        "digits = \"([0-9])\"\n",
        "multiple_dots = r'\\.{2,}'\n",
        "\n",
        "def split_into_sentences(text: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    Split the text into sentences.\n",
        "\n",
        "    If the text contains substrings \"<prd>\" or \"<stop>\", they would lead\n",
        "    to incorrect splitting because they are used as markers for splitting.\n",
        "\n",
        "    :param text: text to be split into sentences\n",
        "    :type text: str\n",
        "\n",
        "    :return: list of sentences\n",
        "    :rtype: list[str]\n",
        "    \"\"\"\n",
        "    text = \" \" + text + \"  \"\n",
        "    text = text.replace(\"\\n\",\" \")\n",
        "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
        "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
        "    text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text)\n",
        "    text = re.sub(multiple_dots, lambda match: \"<prd>\" * len(match.group(0)) + \"<stop>\", text)\n",
        "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
        "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
        "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
        "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
        "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
        "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
        "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
        "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
        "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
        "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
        "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
        "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
        "    text = text.replace(\".\",\".<stop>\")\n",
        "    text = text.replace(\"?\",\"?<stop>\")\n",
        "    text = text.replace(\"!\",\"!<stop>\")\n",
        "    text = text.replace(\"<prd>\",\".\")\n",
        "    sentences = text.split(\"<stop>\")\n",
        "    sentences = [s.strip() for s in sentences]\n",
        "    if sentences and not sentences[-1]: sentences = sentences[:-1]\n",
        "    return sentences\n",
        "\n",
        "\n",
        "def get_sentences(text, lang):\n",
        "    \"\"\"\n",
        "    Crea una pipeline Stanza per una lingua specifica, segmenta il testo in frasi e restituisce la lista di frasi.\n",
        "    \"\"\"\n",
        "    stanza.download(lang)\n",
        "\n",
        "    nlp = stanza.Pipeline(lang, processors='tokenize', use_gpu=True, verbose=False)\n",
        "\n",
        "    doc = nlp(text)\n",
        "\n",
        "    sentences = [sentence.text for sentence in doc.sentences]\n",
        "\n",
        "    return sentences\n",
        "\n",
        "\n",
        "\n",
        "def GetFraseDaAnalizzare(start, end, article, usa_stanza, lang):\n",
        "    # Segmenta il testo\n",
        "    if usa_stanza:\n",
        "        sentences = get_sentences(article, lang)\n",
        "    else:\n",
        "        sentences = split_into_sentences(article)\n",
        "\n",
        "    current_position = 0\n",
        "    for sentence in sentences:\n",
        "        sentence_length = len(sentence)\n",
        "        if current_position <= start < current_position + sentence_length:\n",
        "            return (sentence.strip())\n",
        "        current_position += sentence_length + 1\n",
        "\n",
        "    print(\"Nessuna frase trovata per offset:\", start, end)\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def GetFraseDaAnalizzareConContesto(start, end, article, usa_stanza, lang):\n",
        "    if usa_stanza:\n",
        "        sentences = get_sentences(article, lang)\n",
        "    else:\n",
        "        sentences = split_into_sentences(article)\n",
        "\n",
        "    current_position = 0\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        sentence_length = len(sentence)\n",
        "        if current_position <= start < current_position + sentence_length:\n",
        "            previous_sentence = sentences[i - 1].strip() if i > 0 else \"\"\n",
        "            next_sentence = sentences[i + 1].strip() if i < len(sentences) - 1 else \"\"\n",
        "            target_sentence = sentence.strip()\n",
        "\n",
        "            combined_text = f\"{previous_sentence} {target_sentence} {next_sentence}\".strip()\n",
        "            return combined_text\n",
        "        current_position += sentence_length + 1\n",
        "    print(\"Nessuna frase trovata per offset:\", start, end)\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "\n",
        "def GetLingua(titolo):\n",
        "  lingua='en'\n",
        "  titolo=titolo.lower()\n",
        "  if 'bg' in titolo:\n",
        "    lingua='bg'\n",
        "  elif 'en' in titolo:\n",
        "    lingua='en'\n",
        "  elif 'hi' in titolo:\n",
        "    lingua='hi'\n",
        "  elif 'pt' in titolo:\n",
        "    lingua='pt'\n",
        "  else:\n",
        "    lingua='ru'\n",
        "\n",
        "  return lingua"
      ],
      "metadata": {
        "id": "qhxTvl77xPWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##PROCEDURA PER OTTENERE IL DATASET FINALE PER L'ADDESTRAMENTO\n",
        "import csv\n",
        "import re\n",
        "lingua='EN'\n",
        "path_annotations=\"subtask-1-annotations.txt\"\n",
        "rows=[]\n",
        "\n",
        "def clean_text_advanced(text):\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
        "\n",
        "    text = re.sub(r'[^\\w\\s.,!?]', '', text)\n",
        "\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "with open(path_annotations, 'r', encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "        parts = line.strip().split('\\t')\n",
        "\n",
        "        if len(parts) > 6:\n",
        "            parts = parts[:5] + [' '.join(parts[5:])]\n",
        "        rows.append(parts)\n",
        "\n",
        "df_split = pd.DataFrame(rows, columns=[\"File\", \"Name\", \"Start\", \"End\",\"Role\",\"Descriptor\"])\n",
        "righe_df_finale=[]\n",
        "print(df_split)\n",
        "for index, row in df_split.iterrows():\n",
        "      path=row[\"File\"]\n",
        "\n",
        "      if row[\"Name\"] is None:\n",
        "        continue\n",
        "      nome=row[\"Name\"].lower()\n",
        "      start=row[\"Start\"]\n",
        "      end=row[\"End\"]\n",
        "      role=row[\"Role\"].lower()\n",
        "      descriptor=row[\"Descriptor\"].lower()\n",
        "      try:\n",
        "        with open(f'articoli/{path}', \"r\", encoding=\"utf-8\") as file:\n",
        "          articolo = file.read()\n",
        "      except:\n",
        "        continue\n",
        "\n",
        "      lingua=GetLingua(path)\n",
        "      frase=GetFraseDaAnalizzareConContesto(int(start),int(end),articolo,True,lingua)\n",
        "      frase=newTranslator(frase)\n",
        "      nome=newTranslator(nome)\n",
        "      if frase==\"\":\n",
        "        continue\n",
        "\n",
        "      print(f\"{frase}\\n{nome}\\n\\n----------------\")\n",
        "      #INSERISCO frase, nome, start, end, role, descriptor in un nuovo dataframe risultante\n",
        "      righe_df_finale.append([frase, nome, start, end,role,descriptor])\n",
        "      print(index)\n",
        "\n",
        "\n",
        "df = pd.DataFrame(righe_df_finale, columns=[\"Frase\", \"Name\", \"Start\", \"End\",\"Role\",\"Descriptor\"])\n",
        "df.to_csv(f'train_dataset_finale_{lingua}.csv', index=False, quoting=csv.QUOTE_ALL)\n",
        "\n",
        "print(len(df))"
      ],
      "metadata": {
        "id": "uIvDlNmhxwCO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}