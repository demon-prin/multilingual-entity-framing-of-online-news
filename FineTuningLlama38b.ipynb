{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbziBIr_mPIl"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install -U bitsandbytes\n",
        "%pip install -U transformers\n",
        "%pip install -U accelerate\n",
        "%pip install -U peft\n",
        "%pip install -U trl == 0.12.0\n",
        "!pip install stanza\n",
        "!pip uninstall googletrans\n",
        "\n",
        "!pip install langdetect\n",
        "!pip install -U deep-translator\n",
        "!pip install googletrans==4.0.0-rc1\n",
        "!pip install google_trans_new\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install trl==0.12.0\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import bitsandbytes as bnb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "from datasets import Dataset\n",
        "from peft import LoraConfig, PeftConfig\n",
        "from trl import SFTTrainer\n",
        "from trl import setup_chat_format\n",
        "from transformers import (AutoModelForCausalLM,\n",
        "                          AutoTokenizer,\n",
        "                          BitsAndBytesConfig,\n",
        "                          TrainingArguments,\n",
        "                          pipeline,\n",
        "                          logging)\n",
        "from sklearn.metrics import (accuracy_score,\n",
        "                             classification_report,\n",
        "                             confusion_matrix)\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,AutoModelForSequenceClassification\n",
        "import bitsandbytes as bnb\n",
        "from deep_translator import GoogleTranslator\n",
        "import time"
      ],
      "metadata": {
        "id": "9vi0aRm5mUWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "id": "kluIjc1XmcS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_annotations = f\"dati.csv\"\n",
        "df = pd.read_csv(path_annotations, usecols=[\"Frase\", \"Name\", \"Start\", \"End\", \"Role\", \"Descriptor\"])\n",
        "\n",
        "\n",
        "df_protagonist = pd.DataFrame(columns=['statement','status'])\n",
        "for index, row in df.iterrows():\n",
        "  sub_classi=row['Descriptor'].split()\n",
        "  entity = row['Name'].lower()\n",
        "  sentence = row['Frase']\n",
        "  full_input = f\"Entity: {entity} [SEP] Sentence: {sentence}\"\n",
        "\n",
        "  if 'foreign' in sub_classi:\n",
        "    sub_classi.remove('foreign')\n",
        "    sub_classi.remove('adversary')\n",
        "    sub_classi.append('Foreign Adversary')\n",
        "\n",
        "  sub_classi = [f\"{pr.capitalize()}\" for pr in sub_classi]\n",
        "  nuove_righe=[]\n",
        "\n",
        "  for sb in sub_classi:\n",
        "    new_row = {\n",
        "        'statement': full_input,\n",
        "        'status': sb\n",
        "    }\n",
        "    nuove_righe.append(new_row)\n",
        "\n",
        "  temp_df = pd.DataFrame(nuove_righe)\n",
        "  df_protagonist = pd.concat([df_protagonist, temp_df], ignore_index=True)\n",
        "\n",
        "df=df_protagonist\n",
        "df"
      ],
      "metadata": {
        "id": "HGpkNmL_m4JN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle the DataFrame and select only 3000 rows\n",
        "df = df.sample(frac=1, random_state=85).reset_index(drop=True).head(3000)\n",
        "\n",
        "# Split the DataFrame\n",
        "train_size =0.95\n",
        "eval_size = 0.05\n",
        "\n",
        "# Calculate sizes\n",
        "train_end = int(train_size * len(df))\n",
        "eval_end = train_end + int(eval_size * len(df))\n",
        "\n",
        "# Split the data\n",
        "X_train = df[:train_end]\n",
        "X_eval = df[train_end:eval_end]\n",
        "X_test = df[eval_end:]\n",
        "\n",
        "# Define the prompt generation functions\n",
        "def generate_prompt(data_point):\n",
        "    dati=data_point[\"statement\"].split(\"[SEP]\")\n",
        "    return f\"\"\" You are a multi-label classifier.\n",
        "            Given the following sentence:{dati[1].replace('Sentence:','')}\n",
        "            Choose the labels that belong to the following entity: {dati[0].replace('Entity:','')}\n",
        "            List of possible labels: 'Instigator','Conspirator','Tyrant','Foreign Adversary','Traitor','Spy','Saboteur','Corrupt','Incompetent','Terrorist','Deceiver','Bigot','Guardian','Martyr','Peacemaker','Rebel','Underdog','Virtuous','Forgotten','Exploited','Victim','Scapegoat'.\n",
        "            Don't give me explanations, don't choose more than two labels\n",
        "labels: {data_point[\"status\"]}\"\"\".strip()\n",
        "\n",
        "def generate_test_prompt(data_point):\n",
        "    dati=data_point[\"statement\"].split(\"[SEP]\")\n",
        "    return f\"\"\" You are a multi-label classifier.\n",
        "            Given the following sentence:{dati[1].replace('Sentence:','')}\n",
        "            Choose the labels that belong to the following entity: {dati[0].replace('Entity:','')}\n",
        "            List of possible labels: 'Instigator','Conspirator','Tyrant','Foreign Adversary','Traitor','Spy','Saboteur','Corrupt','Incompetent','Terrorist','Deceiver','Bigot','Guardian','Martyr','Peacemaker','Rebel','Underdog','Virtuous','Forgotten','Exploited','Victim','Scapegoat'.\n",
        "            Don't give me explanations, don't choose more than two labels\n",
        "labels: \"\"\".strip()\n",
        "\n",
        "# Generate prompts for training and evaluation data\n",
        "X_train.loc[:,'text'] = X_train.apply(generate_prompt, axis=1)\n",
        "X_eval.loc[:,'text'] = X_eval.apply(generate_prompt, axis=1)\n",
        "\n",
        "# Generate test prompts and extract true labels\n",
        "y_true = X_test.loc[:,'status']\n",
        "X_test = pd.DataFrame(X_test.apply(generate_test_prompt, axis=1), columns=[\"text\"])\n",
        "\n",
        "X_train.status.value_counts()"
      ],
      "metadata": {
        "id": "TpTm9ikinFbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to datasets\n",
        "train_data = Dataset.from_pandas(X_train[[\"text\"]])\n",
        "eval_data = Dataset.from_pandas(X_eval[[\"text\"]])\n",
        "train_data['text'][3]"
      ],
      "metadata": {
        "id": "epz2O6vynT9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "# Configurazione di BitsAndBytes per la quantizzazione 4-bit\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=\"float16\",\n",
        ")\n",
        "\n",
        "# Caricamento del modello con quantizzazione 4-bit\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"float16\",\n",
        "    quantization_config=bnb_config,\n",
        ")\n",
        "\n",
        "model.config.problem_type = \"multi_label_classification\"\n",
        "model.config.num_labels = 22\n",
        "\n",
        "# Configurazioni del modello\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "# Caricamento del tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "\n",
        "# Imposta il token di padding come il token di fine sequenza\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id"
      ],
      "metadata": {
        "id": "guvDYVacnWw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_all_linear_names(model):\n",
        "    cls = bnb.nn.Linear4bit\n",
        "    lora_module_names = set()\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, cls):\n",
        "            names = name.split('.')\n",
        "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
        "    if 'lm_head' in lora_module_names:  # needed for 16 bit\n",
        "        lora_module_names.remove('lm_head')\n",
        "    return list(lora_module_names)\n",
        "modules = find_all_linear_names(model)\n",
        "modules"
      ],
      "metadata": {
        "id": "hNvBA6rvnfKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir=\"llama-3.1-fine-tuned-model\"\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    r=6,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=modules,\n",
        ")\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    logging_steps=1,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.001,\n",
        "    fp16=True,\n",
        "    bf16=False,\n",
        "    max_grad_norm=0.3,\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=0.03,\n",
        "    group_by_length=False,\n",
        "    lr_scheduler_type=\"cosine\"\n",
        "    report_to=\"wandb\",\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps = 0.2\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_arguments,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=eval_data,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    tokenizer=tokenizer,\n",
        "    max_seq_length=512,\n",
        "    packing=False,\n",
        "    dataset_kwargs={\n",
        "    \"add_special_tokens\": False,\n",
        "    \"append_concat_token\": False,\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "Ry61EGlini5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "A7MhyIg2oCdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predictSingola(testo, model, tokenizer):\n",
        "    y_pred = []\n",
        "    categories = ['Instigator','Conspirator','Tyrant','Foreign Adversary','Traitor','Spy','Saboteur','Corrupt','Incompetent','Terrorist','Deceiver','Bigot','Guardian','Martyr','Peacemaker','Rebel','Underdog','Virtuous','Forgotten','Exploited','Victim','Scapegoat']\n",
        "\n",
        "    prompt = generate_test_prompt(testo)\n",
        "    pipe = pipeline(task=\"text-generation\",\n",
        "                        model=model,\n",
        "                        tokenizer=tokenizer,\n",
        "                        max_new_tokens=3,\n",
        "                        temperature=0.1)\n",
        "\n",
        "    result = pipe(prompt,return_full_text=False)\n",
        "    result[0]['generated_text']=result[0]['generated_text'].replace(prompt,'')\n",
        "    answer = result[0]['generated_text'].split(\"label:\")[-1].strip()\n",
        "    labels=result[0]['generated_text'].split(\"label:\")\n",
        "    labels_predette=' '.join(labels)\n",
        "    # Determine the predicted category\n",
        "    #DA RIVEDERE\n",
        "    for category in categories:\n",
        "        #answer.lower()\n",
        "        if category.lower() in labels_predette.lower():\n",
        "          y_pred.append(category)\n",
        "    else:\n",
        "          y_pred.append(\"none\")\n",
        "\n",
        "    return y_pred\n",
        "\n",
        "\n",
        "\n",
        "def generate_test_prompt(testo):\n",
        "    dati=testo.split(\"[SEP]\")\n",
        "    return f\"\"\" You are a multi-label classifier.\n",
        "            Given the following sentence:{dati[1].replace('Sentence:','')}\n",
        "            Choose the labels that belong to the following entity: {dati[0].replace('Entity:','')}\n",
        "            List of possible labels: 'Instigator','Conspirator','Tyrant','Foreign Adversary','Traitor','Spy','Saboteur','Corrupt','Incompetent','Terrorist','Deceiver','Bigot','Guardian','Martyr','Peacemaker','Rebel','Underdog','Virtuous','Forgotten','Exploited','Victim','Scapegoat'.\n",
        "            Don't give me explanations, don't choose more than two labels\n",
        "            labels: \"\"\".strip()"
      ],
      "metadata": {
        "id": "kM8SCT-2oG0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_by_punctuation(text):\n",
        "    tokens = re.split(r'([.!?]+)', text)\n",
        "\n",
        "    phrases = []\n",
        "    for i in range(0, len(tokens)-1, 2):\n",
        "        sentence = tokens[i].strip() + tokens[i+1]\n",
        "        phrases.append(sentence.strip())\n",
        "    return phrases\n",
        "\n",
        "def tokenize_text(text):\n",
        "    tokens = text.splitlines()\n",
        "    tokenized_list = []\n",
        "\n",
        "    for token in tokens:\n",
        "        sub_tokens = token.split('|')\n",
        "        for sub_token in sub_tokens:\n",
        "            cleaned_token = sub_token.strip()\n",
        "            if cleaned_token:\n",
        "                tokenized_list.append(cleaned_token)\n",
        "    final_tokenized_list = []\n",
        "    for frase in tokenized_list:\n",
        "        sub_phrases = re.split(r'(?<=[.!?।])\\s+', frase)\n",
        "        final_tokenized_list.extend([s.strip() for s in sub_phrases if s.strip()])\n",
        "\n",
        "    return final_tokenized_list\n",
        "\n",
        "def trova_frase_contenente_stringa2(testo, start, end,isHI):\n",
        "  if isHI:\n",
        "      frasi=tokenize_text(testo)\n",
        "      return trova_frase_con_parola(testo, frasi, start, end)\n",
        "  else:\n",
        "      frasi=tokenize_by_punctuation(testo)\n",
        "      return trova_frase_con_parola(testo, frasi, start, end)\n",
        "\n",
        "\n",
        "def trova_frase_con_parola(testo, frasi, inizio_parola, fine_parola):\n",
        "    for frase in frasi:\n",
        "      inizio_frase = testo.find(frase)\n",
        "      fine_frase = inizio_frase + len(frase)\n",
        "      if inizio_parola >= inizio_frase and fine_parola <= fine_frase:\n",
        "          return (frase)\n",
        "    return ''\n",
        "\n",
        "  return sub_predette.issubset(sub_corrette) or PredizioneCorretta(descriptor,predizione)\n",
        "\n",
        "def GetFraseDaAnalizzareConContesto(start, end, article, usa_stanza, lang):\n",
        "    \"\"\"\n",
        "    Estrai la frase contenente un segmento di testo dato (start, end) insieme alle due frasi vicine.\n",
        "    \"\"\"\n",
        "    if usa_stanza:\n",
        "        sentences = get_sentences(article, lang)\n",
        "    else:\n",
        "        sentences = split_into_sentences(article)\n",
        "\n",
        "    current_position = 0\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        sentence_length = len(sentence)\n",
        "        if current_position <= start < current_position + sentence_length:\n",
        "            previous_sentence = sentences[i - 1].strip() if i > 0 else \"\"\n",
        "            next_sentence = sentences[i + 1].strip() if i < len(sentences) - 1 else \"\"\n",
        "            target_sentence = sentence.strip()\n",
        "\n",
        "            combined_text = f\"{previous_sentence} {target_sentence} {next_sentence}\".strip()\n",
        "            return combined_text\n",
        "        current_position += sentence_length + 1\n",
        "\n",
        "    # Nessuna frase trovata\n",
        "    print(\"Nessuna frase trovata per offset:\", start, end)\n",
        "    return \"\""
      ],
      "metadata": {
        "id": "tyo_YCQOpKoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stanza\n",
        "import pandas as pd\n",
        "import stanza\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "import re\n",
        "alphabets= \"([A-Za-z])\"\n",
        "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
        "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
        "starters = \"(Mr|Mrs|Ms|Dr|Prof|Capt|Cpt|Lt|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
        "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
        "websites = \"[.](com|net|org|io|gov|edu|me)\"\n",
        "digits = \"([0-9])\"\n",
        "multiple_dots = r'\\.{2,}'\n",
        "\n",
        "def split_into_sentences(text: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    Split the text into sentences.\n",
        "\n",
        "    If the text contains substrings \"<prd>\" or \"<stop>\", they would lead\n",
        "    to incorrect splitting because they are used as markers for splitting.\n",
        "\n",
        "    :param text: text to be split into sentences\n",
        "    :type text: str\n",
        "\n",
        "    :return: list of sentences\n",
        "    :rtype: list[str]\n",
        "    \"\"\"\n",
        "    text = \" \" + text + \"  \"\n",
        "    text = text.replace(\"\\n\",\" \")\n",
        "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
        "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
        "    text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text)\n",
        "    text = re.sub(multiple_dots, lambda match: \"<prd>\" * len(match.group(0)) + \"<stop>\", text)\n",
        "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
        "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
        "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
        "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
        "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
        "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
        "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
        "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
        "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
        "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
        "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
        "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
        "    text = text.replace(\".\",\".<stop>\")\n",
        "    text = text.replace(\"?\",\"?<stop>\")\n",
        "    text = text.replace(\"!\",\"!<stop>\")\n",
        "    text = text.replace(\"<prd>\",\".\")\n",
        "    sentences = text.split(\"<stop>\")\n",
        "    sentences = [s.strip() for s in sentences]\n",
        "    if sentences and not sentences[-1]: sentences = sentences[:-1]\n",
        "    return sentences\n",
        "\n",
        "\n",
        "def get_sentences(text, lang):\n",
        "    \"\"\"\n",
        "    Crea una pipeline Stanza per una lingua specifica, segmenta il testo in frasi e restituisce la lista di frasi.\n",
        "\n",
        "    Args:\n",
        "        text (str): Il testo da segmentare.\n",
        "        lang (str): Codice ISO-639-1 della lingua (es. 'en', 'ru', 'fr', ecc.).\n",
        "\n",
        "    Returns:\n",
        "        list: Lista di frasi segmentate.\n",
        "    \"\"\"\n",
        "    stanza.download(lang)\n",
        "\n",
        "    nlp = stanza.Pipeline(lang, processors='tokenize', use_gpu=True, verbose=False)\n",
        "\n",
        "    doc = nlp(text)\n",
        "\n",
        "    sentences = [sentence.text for sentence in doc.sentences]\n",
        "\n",
        "    return sentences\n",
        "\n",
        "\n",
        "def GetLingua(titolo):\n",
        "  lingua='en'\n",
        "  titolo=titolo.lower()\n",
        "  if 'bg' in titolo:\n",
        "    lingua='bg'\n",
        "  elif 'en' in titolo:\n",
        "    lingua='en'\n",
        "  elif 'hi' in titolo:\n",
        "    lingua='hi'\n",
        "  elif 'pt' in titolo:\n",
        "    lingua='pt'\n",
        "  else:\n",
        "    lingua='ru'\n",
        "\n",
        "  return lingua\n",
        "\n",
        "\n",
        "def Traduci(testo, lingua):\n",
        "  return GoogleTranslator(source='pt', target='en').translate(testo)"
      ],
      "metadata": {
        "id": "kRhFdoL4pyR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FASE DI VALIDAZIONE\n",
        "#INPUT: file con le annotazioni, cartello 'articoli' con l'insieme di articoli da analizzare\n",
        "import re\n",
        "# Leggi le annotazioni\n",
        "rows_test=[]\n",
        "path_annotations = f\"subtask-1-entity-mentions.txt\"\n",
        "\n",
        "with open(path_annotations, 'r', encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "        parts = line.strip().split('\\t')\n",
        "\n",
        "        if len(parts) > 6:\n",
        "            parts = parts[:5] + [' '.join(parts[5:])]\n",
        "\n",
        "        rows_test.append(parts)\n",
        "\n",
        "df_righeEval = pd.DataFrame(rows_test, columns=[\"File\", \"Name\", \"Start\", \"End\"])\n",
        "\n",
        "df_evaluation = pd.DataFrame(columns=['input','Instigator','Conspirator','Tyrant','Foreign Adversary','Traitor','Spy','Saboteur','Corrupt','Incompetent','Terrorist','Deceiver','Bigot','Forgotten','Exploited','Victim','Scapegoat'\n",
        ",'Guardian'\n",
        ",'Martyr'\n",
        ",'Peacemaker'\n",
        ",'Rebel'\n",
        ",'Underdog'\n",
        ",'Virtuous'\n",
        "])\n",
        "\n",
        "for index, row in df_righeEval.iterrows():\n",
        "        path = row[\"File\"]\n",
        "        nome = row[\"Name\"].strip()\n",
        "        start = row[\"Start\"]\n",
        "        end = row[\"End\"]\n",
        "\n",
        "        try:\n",
        "            with open(f'articoli/{path}', \"r\", encoding=\"utf-8\") as file:\n",
        "                articolo = file.read()\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nell'apertura del file {path}: {e}\")\n",
        "            continue\n",
        "\n",
        "        lingua=\"PT\" #lingua di origine degli articoli\n",
        "        frase=GetFraseDaAnalizzareConContesto(int(start),int(end),articolo,True,lingua)\n",
        "\n",
        "        nome_tradotto=Traduci(nome,'pt')\n",
        "        frase_tradotta=Traduci(frase,'pt')\n",
        "        full_input = f\"Entity: {nome_tradotto} [SEP] Sentence: {frase_tradotta}\"\n",
        "\n",
        "        sub=(predictSingola(full_input, model, tokenizer))\n",
        "\n",
        "        if sub[0]=='none':\n",
        "          sub=[]\n",
        "\n",
        "        if 'none' in sub:\n",
        "          sub.remove('none')\n",
        "\n",
        "        #prendiamo una sola etichetta\n",
        "        if len(sub)>1:\n",
        "          sub = [sub[-1]]\n",
        "\n",
        "        #ricerca del ruolo principale\n",
        "        prima_sub=sub[0] if len(sub)>0 else ''\n",
        "        classe='Antagonist'\n",
        "        if prima_sub in ['Forgotten','Exploited','Victim','Scapegoat']:\n",
        "          classe='Innocent'\n",
        "        elif prima_sub in ['Guardian','Martyr','Peacemaker','Rebel','Underdog','Virtuous']:\n",
        "          classe='Protagonist'\n",
        "\n",
        "        with open('output', \"a\", encoding=\"utf-8\") as out_file:\n",
        "            line = f\"{path}\\t{row['Name']}\\t{start}\\t{end}\\t{classe}\\t\" + '\\t'.join(sub) + \"\\n\"\n",
        "            out_file.write(line)"
      ],
      "metadata": {
        "id": "zaHWLIOzvZNB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}